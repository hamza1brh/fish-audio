{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAudio S1-Mini LoRA Finetuning\n",
        "\n",
        "This notebook follows the **official finetuning guide** exactly as documented.\n",
        "\n",
        "Key fix: LoRA weight initialization (preserves pretrained weights).\n",
        "\n",
        "## Requirements\n",
        "- CUDA-enabled GPU with 12GB+ VRAM\n",
        "- Dataset at `data/neymar_finetune/` with `.wav`, `.lab`, `.npy` files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project Root: C:\\Users\\PC\\Desktop\\fish-speech\n",
            "PyTorch: 2.9.1+cu130\n",
            "CUDA: True\n",
            "GPU: NVIDIA GeForce RTX 5070 Ti\n",
            "VRAM: 15.9 GB\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Environment Setup\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "def find_project_root() -> Path:\n",
        "    current = Path(os.getcwd()).resolve()\n",
        "    for _ in range(10):\n",
        "        if (current / \"fish_speech\").exists():\n",
        "            return current\n",
        "        if current == current.parent:\n",
        "            break\n",
        "        current = current.parent\n",
        "    \n",
        "    common_paths = [\n",
        "        Path.home() / \"Desktop\" / \"fish-speech\",\n",
        "        Path.home() / \"fish-speech\",\n",
        "    ]\n",
        "    for p in common_paths:\n",
        "        if (p / \"fish_speech\").exists():\n",
        "            return p\n",
        "    raise RuntimeError(f\"Could not find fish_speech directory\")\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"VRAM: {vram:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: C:\\Users\\PC\\Desktop\\fish-speech\\data\\neymar_finetune\n",
            "Protos: C:\\Users\\PC\\Desktop\\fish-speech\\data\\neymar_finetune\\protos\n",
            "Base Model: C:\\Users\\PC\\Desktop\\fish-speech\\checkpoints\\openaudio-s1-mini\n",
            "Output: C:\\Users\\PC\\Desktop\\fish-speech\\checkpoints\\openaudio-s1-mini-neymar_lora\n",
            "Training: 1000 steps, batch=4, lr=0.0001\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Configuration\n",
        "# Following official docs exactly\n",
        "\n",
        "# Dataset path - should have speaker subfolders (SPK1, SPK2, etc.)\n",
        "# Or all files in one folder (treated as one speaker)\n",
        "DATASET_PATH = PROJECT_ROOT / \"data/neymar_finetune\"\n",
        "\n",
        "# Model path\n",
        "BASE_MODEL = PROJECT_ROOT / \"checkpoints/openaudio-s1-mini\"\n",
        "\n",
        "# Training config (following official docs)\n",
        "PROJECT_NAME = \"neymar_lora\"\n",
        "MAX_STEPS = 1000\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-4\n",
        "VAL_INTERVAL = 100\n",
        "\n",
        "# Output\n",
        "OUTPUT_MODEL = PROJECT_ROOT / f\"checkpoints/openaudio-s1-mini-{PROJECT_NAME}\"\n",
        "RESULTS_DIR = PROJECT_ROOT / f\"results/{PROJECT_NAME}\"\n",
        "\n",
        "# Protos path (official location)\n",
        "PROTOS_PATH = DATASET_PATH / \"protos\"\n",
        "\n",
        "# Validate\n",
        "assert DATASET_PATH.exists(), f\"Dataset not found: {DATASET_PATH}\"\n",
        "assert BASE_MODEL.exists(), f\"Base model not found: {BASE_MODEL}\"\n",
        "\n",
        "print(f\"Dataset: {DATASET_PATH}\")\n",
        "print(f\"Protos: {PROTOS_PATH}\")\n",
        "print(f\"Base Model: {BASE_MODEL}\")\n",
        "print(f\"Output: {OUTPUT_MODEL}\")\n",
        "print(f\"Training: {MAX_STEPS} steps, batch={BATCH_SIZE}, lr={LEARNING_RATE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VQ tokens already extracted: 742 files found\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Extract VQ Tokens (if not already done)\n",
        "# Official command from docs/en/finetune.md\n",
        "\n",
        "npy_files = list(DATASET_PATH.rglob(\"*.npy\"))\n",
        "if npy_files:\n",
        "    print(f\"VQ tokens already extracted: {len(npy_files)} files found\")\n",
        "else:\n",
        "    print(\"Extracting VQ tokens...\")\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"tools/vqgan/extract_vq.py\",\n",
        "        str(DATASET_PATH),\n",
        "        \"--num-workers\", \"1\",\n",
        "        \"--batch-size\", \"8\",\n",
        "        \"--config-name\", \"modded_dac_vq\",\n",
        "        \"--checkpoint-path\", str(BASE_MODEL / \"codec.pth\"),\n",
        "    ]\n",
        "    print(f\"Running: {' '.join(cmd)}\")\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\", cwd=PROJECT_ROOT)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"ERROR: {result.stderr}\")\n",
        "    else:\n",
        "        print(\"VQ extraction complete\")\n",
        "        npy_files = list(DATASET_PATH.rglob(\"*.npy\"))\n",
        "        print(f\"Created {len(npy_files)} .npy files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Protobuf dataset already exists at C:\\Users\\PC\\Desktop\\fish-speech\\data\\neymar_finetune\\protos\n",
            "  1 files, 1.9 MB\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Build Protobuf Dataset\n",
        "# Official command from docs/en/finetune.md\n",
        "\n",
        "if PROTOS_PATH.exists() and list(PROTOS_PATH.glob(\"*.protos\")):\n",
        "    print(f\"Protobuf dataset already exists at {PROTOS_PATH}\")\n",
        "    protos = list(PROTOS_PATH.glob(\"*.protos\"))\n",
        "    total_size = sum(p.stat().st_size for p in protos)\n",
        "    print(f\"  {len(protos)} files, {total_size / 1024 / 1024:.1f} MB\")\n",
        "else:\n",
        "    print(\"Building protobuf dataset...\")\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"tools/llama/build_dataset.py\",\n",
        "        \"--input\", str(DATASET_PATH),\n",
        "        \"--output\", str(PROTOS_PATH),\n",
        "        \"--text-extension\", \".lab\",\n",
        "        \"--num-workers\", \"4\",\n",
        "    ]\n",
        "    print(f\"Running: {' '.join(cmd)}\")\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\", cwd=PROJECT_ROOT)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"ERROR: {result.stderr}\")\n",
        "    else:\n",
        "        print(\"Protobuf dataset built successfully\")\n",
        "        protos = list(PROTOS_PATH.glob(\"*.protos\"))\n",
        "        print(f\"Created {len(protos)} .protos files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Protos path: C:\\Users\\PC\\Desktop\\fish-speech\\data\\neymar_finetune\\protos\n",
            "Protos exists: True\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "C:UsersPCDesktopfish-speechdataneymar_finetuneprotos is not a file or directory",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m FishTokenizer(\u001b[38;5;28mstr\u001b[39m(BASE_MODEL \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m AutoTextSemanticInstructionIterableDataset(\n\u001b[0;32m     12\u001b[0m     proto_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(PROTOS_PATH)],\n\u001b[0;32m     13\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     interactive_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample tokens shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\Desktop\\fish-speech\\fish_speech\\datasets\\semantic.py:116\u001b[0m, in \u001b[0;36mAutoTextSemanticInstructionIterableDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\Desktop\\fish-speech\\fish_speech\\datasets\\semantic.py:253\u001b[0m, in \u001b[0;36mAutoTextSemanticInstructionIterableDataset.augment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maugment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 253\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response\u001b[38;5;241m.\u001b[39msamples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# Invalid group\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\Desktop\\fish-speech\\fish_speech\\datasets\\semantic.py:159\u001b[0m, in \u001b[0;36mAutoTextSemanticInstructionIterableDataset.sample_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_mock_data_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Shuffle unique lines, estimate that each sample is at least 20 tokens\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
            "File \u001b[1;32m~\\Desktop\\fish-speech\\fish_speech\\datasets\\semantic.py:133\u001b[0m, in \u001b[0;36mAutoTextSemanticInstructionIterableDataset.init_mock_data_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m             expanded_proto_files\u001b[38;5;241m.\u001b[39mextend(i\u001b[38;5;241m.\u001b[39mrglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.protos\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a file or directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m expanded_proto_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(expanded_proto_files)\n\u001b[0;32m    136\u001b[0m Random(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\u001b[38;5;241m.\u001b[39mshuffle(expanded_proto_files)\n",
            "\u001b[1;31mValueError\u001b[0m: C:UsersPCDesktopfish-speechdataneymar_finetuneprotos is not a file or directory"
          ]
        }
      ],
      "source": [
        "# Cell 5: Verify Dataset\n",
        "# Check the dataset loads correctly using official dataset class\n",
        "\n",
        "from fish_speech.datasets.semantic import AutoTextSemanticInstructionIterableDataset\n",
        "from fish_speech.tokenizer import FishTokenizer\n",
        "\n",
        "print(f\"Protos path: {PROTOS_PATH}\")\n",
        "print(f\"Protos exists: {PROTOS_PATH.exists()}\")\n",
        "\n",
        "tokenizer = FishTokenizer(str(BASE_MODEL / \"tokenizer.tiktoken\"))\n",
        "dataset = AutoTextSemanticInstructionIterableDataset(\n",
        "    proto_files=[str(PROTOS_PATH)],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=4096,\n",
        "    use_speaker=False,\n",
        "    interactive_prob=0.7,\n",
        ")\n",
        "\n",
        "sample = next(iter(dataset))\n",
        "tokens = sample[\"inputs\"]\n",
        "print(f\"Sample tokens shape: {tokens.shape}\")\n",
        "print(f\"First 50 tokens decoded: {tokenizer.decode(tokens[0, :50].tolist())}\")\n",
        "print(\"[OK] Dataset loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: LoRA Training\n",
        "# Official command from docs/en/finetune.md\n",
        "\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Clear old checkpoints to start fresh\n",
        "ckpt_dir = RESULTS_DIR / \"checkpoints\"\n",
        "if ckpt_dir.exists():\n",
        "    shutil.rmtree(ckpt_dir)\n",
        "    print(f\"Cleared old checkpoints: {ckpt_dir}\")\n",
        "\n",
        "print(f\"Starting training: {MAX_STEPS} steps...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Official training command\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"fish_speech/train.py\",\n",
        "    \"--config-name\", \"text2semantic_finetune\",  # Official config\n",
        "    f\"project={PROJECT_NAME}\",\n",
        "    f\"trainer.max_steps={MAX_STEPS}\",\n",
        "    f\"trainer.val_check_interval={VAL_INTERVAL}\",\n",
        "    f\"data.batch_size={BATCH_SIZE}\",\n",
        "    f\"model.optimizer.lr={LEARNING_RATE}\",\n",
        "    \"+lora@model.model.lora_config=r_8_alpha_16\",\n",
        "    f\"train_dataset.proto_files=[{PROTOS_PATH}]\",\n",
        "    f\"val_dataset.proto_files=[{PROTOS_PATH}]\",\n",
        "    # Windows-specific settings\n",
        "    \"trainer.strategy=auto\",\n",
        "    \"trainer.devices=1\",\n",
        "]\n",
        "\n",
        "print(\"Command:\")\n",
        "print(\" \".join(cmd))\n",
        "print(\"=\"*60)\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    cmd,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    encoding=\"utf-8\",\n",
        "    errors=\"replace\",\n",
        "    cwd=PROJECT_ROOT,\n",
        ")\n",
        "\n",
        "# Stream output\n",
        "if process.stdout:\n",
        "    for line in process.stdout:\n",
        "        print(line, end=\"\")\n",
        "\n",
        "process.wait()\n",
        "print(\"=\"*60)\n",
        "print(f\"Training completed with exit code: {process.returncode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Merge LoRA Weights\n",
        "\n",
        "checkpoints = sorted((RESULTS_DIR / \"checkpoints\").glob(\"*.ckpt\"))\n",
        "if not checkpoints:\n",
        "    print(\"ERROR: No checkpoints found!\")\n",
        "else:\n",
        "    latest_ckpt = checkpoints[-1]\n",
        "    print(f\"Available checkpoints: {[c.name for c in checkpoints]}\")\n",
        "    print(f\"Using: {latest_ckpt.name}\")\n",
        "    \n",
        "    # Clean output dir\n",
        "    if OUTPUT_MODEL.exists():\n",
        "        shutil.rmtree(OUTPUT_MODEL)\n",
        "    \n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"tools/llama/merge_lora.py\",\n",
        "        \"--lora-config\", \"r_8_alpha_16\",\n",
        "        \"--base-weight\", str(BASE_MODEL),\n",
        "        \"--lora-weight\", str(latest_ckpt),\n",
        "        \"--output\", str(OUTPUT_MODEL),\n",
        "    ]\n",
        "    \n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\")\n",
        "    print(result.stdout)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"[OK] Merged model saved to: {OUTPUT_MODEL}\")\n",
        "    else:\n",
        "        print(f\"ERROR: {result.stderr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Test Finetuned Model\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "TEST_TEXT = \"Hello, this is a test of the finetuned voice model.\"\n",
        "OUTPUT_DIR = Path(\"temp/finetuned_test\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Testing model: {OUTPUT_MODEL}\")\n",
        "print(f\"Text: {TEST_TEXT}\")\n",
        "\n",
        "# Generate semantic tokens\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"fish_speech/models/text2semantic/inference.py\",\n",
        "    \"--text\", TEST_TEXT,\n",
        "    \"--checkpoint-path\", str(OUTPUT_MODEL),\n",
        "    \"--max-new-tokens\", \"200\",\n",
        "    \"--output-dir\", str(OUTPUT_DIR),\n",
        "]\n",
        "result = subprocess.run(cmd, capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\")\n",
        "print(result.stdout[-500:] if len(result.stdout) > 500 else result.stdout)\n",
        "\n",
        "codes_file = OUTPUT_DIR / \"codes_0.npy\"\n",
        "if codes_file.exists():\n",
        "    codes = np.load(codes_file)\n",
        "    print(f\"Generated codes shape: {codes.shape}\")\n",
        "    print(f\"Codebook 0 unique values: {len(set(codes[0]))}\")\n",
        "    \n",
        "    # Decode to audio\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"fish_speech/models/dac/inference.py\",\n",
        "        \"-i\", str(codes_file),\n",
        "        \"--checkpoint-path\", str(BASE_MODEL / \"codec.pth\"),\n",
        "        \"-o\", str(OUTPUT_DIR / \"output.wav\"),\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "    \n",
        "    audio_file = OUTPUT_DIR / \"output.wav\"\n",
        "    if audio_file.exists():\n",
        "        print(f\"Audio saved to: {audio_file}\")\n",
        "        display(Audio(filename=str(audio_file)))\n",
        "    else:\n",
        "        print(\"ERROR: Audio generation failed\")\n",
        "else:\n",
        "    print(\"ERROR: Semantic token generation failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Use the finetuned model** at `checkpoints/openaudio-s1-mini-neymar_lora/`\n",
        "2. **For voice cloning**: Use with reference audio from the training speaker\n",
        "3. **Adjust training**: Increase `MAX_STEPS` for better quality (try 2000-5000)\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "- **Loss not decreasing**: Check dataset quality, try lower learning rate\n",
        "- **Gibberish audio**: Ensure `Codebook 0 unique values` > 30 (should be 50+)\n",
        "- **OOM errors**: Reduce `BATCH_SIZE` to 1\n",
        "\n",
        "### What Was Fixed\n",
        "\n",
        "1. **LoRA Weight Init**: `fish_speech/models/text2semantic/lora.py` now copies pretrained weights\n",
        "2. **Data Format**: Uses `InterleaveFormatDataset` matching inference format\n",
        "3. **Dataset Grouping**: Splits data into smaller groups to prevent truncation\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
